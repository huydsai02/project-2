{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/huydsai02/project-2.git\n!pip install -q -r /kaggle/working/project-2/requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-06-12T06:42:53.170781Z","iopub.execute_input":"2023-06-12T06:42:53.171390Z","iopub.status.idle":"2023-06-12T06:43:24.818109Z","shell.execute_reply.started":"2023-06-12T06:42:53.171365Z","shell.execute_reply":"2023-06-12T06:43:24.816851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/project-2/model/apl_update\nfrom model import APL\n%cd /kaggle/input","metadata":{"execution":{"iopub.status.busy":"2023-06-12T06:43:24.820555Z","iopub.execute_input":"2023-06-12T06:43:24.821146Z","iopub.status.idle":"2023-06-12T06:43:27.836747Z","shell.execute_reply.started":"2023-06-12T06:43:24.821110Z","shell.execute_reply":"2023-06-12T06:43:27.835793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jiwer import wer\nfrom transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\nimport torch, json, os, time, librosa, transformers, gc\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ExponentialLR\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom python_speech_features import fbank\n# from multiprocessing import get_context\nfrom pyctcdecode import build_ctcdecoder\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport warnings","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-12T06:43:27.838239Z","iopub.execute_input":"2023-06-12T06:43:27.839078Z","iopub.status.idle":"2023-06-12T06:43:36.907826Z","shell.execute_reply.started":"2023-06-12T06:43:27.839043Z","shell.execute_reply":"2023-06-12T06:43:36.906916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\nmodel_wav2vec = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")","metadata":{"execution":{"iopub.status.busy":"2023-06-12T06:43:36.910156Z","iopub.execute_input":"2023-06-12T06:43:36.910429Z","iopub.status.idle":"2023-06-12T06:44:08.545952Z","shell.execute_reply.started":"2023-06-12T06:43:36.910405Z","shell.execute_reply":"2023-06-12T06:44:08.545034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/model-project-2/vocab_39.json') as f:\n    dict_vocab = json.load(f)\n\ndef text_to_tensor(text):\n    text = text.lower()\n    text = text.split(\" \")\n    text_list = []\n    for idex in text:\n        text_list.append(dict_vocab[idex])\n    return text_list","metadata":{"execution":{"iopub.status.busy":"2023-06-12T06:44:08.547537Z","iopub.execute_input":"2023-06-12T06:44:08.547918Z","iopub.status.idle":"2023-06-12T06:44:08.560944Z","shell.execute_reply.started":"2023-06-12T06:44:08.547868Z","shell.execute_reply":"2023-06-12T06:44:08.560090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\n\nclass MDD_Dataset(Dataset):\n\n    def __init__(self, data):\n        self.len_data           = len(data)\n        self.path               = list(data['Path'])\n        self.canonical          = list(data['Canonical'])\n        self.transcript         = list(data['Transcript'])\n\n    def __getitem__(self, index):\n        waveform, _ = librosa.load(self.path[index], sr=16000)\n        linguistic  = text_to_tensor(self.canonical[index])\n        transcript  = self.transcript[index]\n        label       = text_to_tensor(transcript)\n        return waveform, linguistic, label, transcript\n\n    def __len__(self):\n        return self.len_data","metadata":{"execution":{"iopub.status.busy":"2023-06-12T06:44:08.562587Z","iopub.execute_input":"2023-06-12T06:44:08.563170Z","iopub.status.idle":"2023-06-12T06:44:08.571011Z","shell.execute_reply.started":"2023-06-12T06:44:08.563140Z","shell.execute_reply":"2023-06-12T06:44:08.569944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/project-2-new-data/csv/train.csv')\ndf_dev = pd.read_csv('/kaggle/input/project-2-new-data/csv/dev.csv')","metadata":{"execution":{"iopub.status.busy":"2023-06-12T06:44:08.572538Z","iopub.execute_input":"2023-06-12T06:44:08.572869Z","iopub.status.idle":"2023-06-12T06:44:08.647956Z","shell.execute_reply.started":"2023-06-12T06:44:08.572840Z","shell.execute_reply":"2023-06-12T06:44:08.647075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"audio_folder = '/kaggle/input/project-2-new-data/audio'","metadata":{"execution":{"iopub.status.busy":"2023-06-12T06:44:08.649750Z","iopub.execute_input":"2023-06-12T06:44:08.650406Z","iopub.status.idle":"2023-06-12T06:44:08.654652Z","shell.execute_reply.started":"2023-06-12T06:44:08.650375Z","shell.execute_reply":"2023-06-12T06:44:08.653748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Path'] = df_train['Path'].apply(lambda x : os.path.join(audio_folder, str(x).zfill(5), 'audio.wav'))\ndf_dev['Path'] = df_dev['Path'].apply(lambda x : os.path.join(audio_folder, str(x).zfill(5), 'audio.wav'))","metadata":{"execution":{"iopub.status.busy":"2023-06-12T06:44:08.656161Z","iopub.execute_input":"2023-06-12T06:44:08.656482Z","iopub.status.idle":"2023-06-12T06:44:08.702598Z","shell.execute_reply.started":"2023-06-12T06:44:08.656453Z","shell.execute_reply":"2023-06-12T06:44:08.701658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2023-06-12T06:44:08.707118Z","iopub.execute_input":"2023-06-12T06:44:08.707383Z","iopub.status.idle":"2023-06-12T06:44:08.724781Z","shell.execute_reply.started":"2023-06-12T06:44:08.707360Z","shell.execute_reply":"2023-06-12T06:44:08.723993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_wer = 100\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_epoch = 150","metadata":{"execution":{"iopub.status.busy":"2023-06-12T06:44:08.726536Z","iopub.execute_input":"2023-06-12T06:44:08.727407Z","iopub.status.idle":"2023-06-12T06:44:08.755431Z","shell.execute_reply.started":"2023-06-12T06:44:08.727377Z","shell.execute_reply":"2023-06-12T06:44:08.754502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = MDD_Dataset(df_train)\ndev_dataset = MDD_Dataset(df_dev)","metadata":{"execution":{"iopub.status.busy":"2023-06-12T06:44:08.757592Z","iopub.execute_input":"2023-06-12T06:44:08.758172Z","iopub.status.idle":"2023-06-12T06:44:08.770051Z","shell.execute_reply.started":"2023-06-12T06:44:08.758137Z","shell.execute_reply":"2023-06-12T06:44:08.768990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_embed = torch.nn.Sequential(*(list(model_wav2vec.children())[:-2])).to(device)\nmodel_embed.eval()\ndel model_wav2vec\ngc.collect()\n\nfor param in model_embed.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2023-06-12T06:44:08.771281Z","iopub.execute_input":"2023-06-12T06:44:08.771736Z","iopub.status.idle":"2023-06-12T06:44:13.953521Z","shell.execute_reply.started":"2023-06-12T06:44:08.771703Z","shell.execute_reply":"2023-06-12T06:44:13.952572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    with torch.no_grad():\n        sr = 16000\n        max_col = [-1] * 3\n        target_length = []\n        for row in batch:\n            if row[0].shape[0] > max_col[0]:\n                max_col[0] = row[0].shape[0]\n            if len(row[1]) > max_col[1]:\n                max_col[1] = len(row[1])\n            if len(row[2]) > max_col[2]:\n                max_col[2] = len(row[2])\n            target_length.append(len(row[2]))\n\n        cols = {'fbank':[], 'linguistic':[], 'labels':[], 'waveform':[], 'transcript':[]}\n        \n        for row in batch:\n            pad_wav = np.concatenate([row[0], np.zeros(max_col[0] - row[0].shape[0])])\n            cols['waveform'].append(pad_wav)\n            melfbank, energy = fbank(pad_wav, sr, winlen=0.02, winstep = 0.02, nfilt=80)\n            cols['fbank'].append(np.concatenate([melfbank, energy.reshape(-1, 1)], axis=1).tolist())\n            row[1].extend([len(dict_vocab)] * (max_col[1] - len(row[1])))\n            cols['linguistic'].append(row[1])\n            row[2].extend([len(dict_vocab)] * (max_col[2] - len(row[2])))\n            cols['labels'].append(row[2])\n            cols['transcript'].append(row[3])\n            \n        inputs = processor(cols['waveform'], return_tensors=\"pt\",sampling_rate=sr, padding=\"longest\")\n        input_values = inputs.input_values.to(device)\n        phonetic = model_embed(input_values).last_hidden_state\n        \n        cols['fbank'] = torch.tensor(cols['fbank'], dtype=torch.float, device=device)[:, :phonetic.shape[1], :]\n        cols['linguistic'] = torch.tensor(cols['linguistic'], dtype=torch.long, device=device)\n        cols['labels'] = torch.tensor(cols['labels'], dtype=torch.long, device=device)\n        targets_length = torch.tensor(target_length, dtype=torch.long, device=device)\n    \n    return cols['fbank'], phonetic, cols['linguistic'], cols['labels'], targets_length, cols['transcript']","metadata":{"execution":{"iopub.status.busy":"2023-06-12T06:46:20.614025Z","iopub.execute_input":"2023-06-12T06:46:20.614389Z","iopub.status.idle":"2023-06-12T06:46:20.630591Z","shell.execute_reply.started":"2023-06-12T06:46:20.614360Z","shell.execute_reply":"2023-06-12T06:46:20.629711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\ndev_loader = DataLoader(dataset=dev_dataset, batch_size=32, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2023-06-12T06:46:17.329069Z","iopub.execute_input":"2023-06-12T06:46:17.329458Z","iopub.status.idle":"2023-06-12T06:46:17.335124Z","shell.execute_reply.started":"2023-06-12T06:46:17.329426Z","shell.execute_reply":"2023-06-12T06:46:17.334105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = APL(len(dict_vocab)).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-06-12T06:46:24.069655Z","iopub.execute_input":"2023-06-12T06:46:24.070039Z","iopub.status.idle":"2023-06-12T06:46:24.189364Z","shell.execute_reply.started":"2023-06-12T06:46:24.070009Z","shell.execute_reply":"2023-06-12T06:46:24.188492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ctc_loss = nn.CTCLoss(blank = len(dict_vocab))\noptimizer = optim.AdamW(model.parameters(), lr = 1.5e-5)\nscheduler = ExponentialLR(optimizer, gamma=0.97)","metadata":{"execution":{"iopub.status.busy":"2023-06-12T06:46:25.416437Z","iopub.execute_input":"2023-06-12T06:46:25.416995Z","iopub.status.idle":"2023-06-12T06:46:25.430109Z","shell.execute_reply.started":"2023-06-12T06:46:25.416956Z","shell.execute_reply":"2023-06-12T06:46:25.429183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_vocab = sorted([w for w in list(dict_vocab.keys())], key=lambda x : dict_vocab[x])\nlist_vocab = [f'{w} ' for w in list_vocab]\nloss_file = open('/kaggle/working/loss_file.txt', 'w')\nprint(\"Start Training\")\n\nfor epoch in range(num_epoch):\n    model.train().to(device)\n    running_loss = []\n    print(f'EPOCH {epoch}:')\n#     print(\"TRAIN:\")\n    t1 = time.time()\n    for i, data in enumerate(train_loader):\n        acoustic, phonetic, linguistic, labels, target_lengths, transcripts  = data\n        \n        optimizer.zero_grad()\n        outputs = model(acoustic, phonetic, linguistic)\n        outputs = outputs.transpose(0, 1)\n        \n        out_shape = outputs.shape\n        \n        input_lengths = torch.full(size=(out_shape[1],), fill_value=out_shape[0], dtype=torch.long, device=device)\n        outputs = (F.log_softmax(outputs, dim=2))\n        \n#         print(outputs.shape)\n#         print(labels.shape)\n#         print(input_lengths)\n#         print(target_lengths)\n            \n#         target_lengths = torch.full(size=(out_shape[1],), fill_value=labels.shape[1], dtype=torch.long, device=device)\n        loss = ctc_loss(outputs, labels, input_lengths, target_lengths)\n        running_loss.append(loss.item())\n        loss_file.write(f\"Epoch {epoch}, train {i} loss: {loss.item()}\\n\")\n        loss.backward()\n        optimizer.step()\n        \n    scheduler.step()\n    print(f\"Training loss: {sum(running_loss) / len(running_loss)}\")\n    \n    running_loss = []\n    with torch.no_grad():\n        model.eval().to(device)\n        worderrorrate = []\n#         print(\"EVAL:\")\n        for i, data in enumerate(dev_loader):\n            acoustic, phonetic, linguistic, labels, target_lengths, transcripts = data\n            outputs = model(acoustic, phonetic, linguistic)\n            \n            outputs = outputs.transpose(0, 1)\n            out_shape = outputs.shape\n            input_lengths = torch.full(size=(out_shape[1],), fill_value=out_shape[0], dtype=torch.long, device=device)\n            outputs = F.log_softmax(outputs, dim=2)\n            loss = ctc_loss(outputs, labels, input_lengths, target_lengths)\n            running_loss.append(loss.item())\n            \n            loss_file.write(f\"Epoch {epoch}, test {i} loss: {loss.item()}\\n\")\n            \n            if epoch > 7:\n                x = outputs.transpose(0, 1)\n                x = x.detach().cpu().numpy()\n                decoder_ctc = build_ctcdecoder(\n                    labels = list_vocab,\n                )\n\n                for n_transcript in range(len(transcripts)):\n                    ground_truth = transcripts[n_transcript]\n                    hypothesis = str(decoder_ctc.decode(x[n_transcript])).strip()\n                    error = wer(ground_truth, hypothesis)\n                    worderrorrate.append(error)\n\n        if epoch > 7:\n            epoch_wer = sum(worderrorrate)/len(worderrorrate)\n\n            with open('/kaggle/working/wer_file.txt', 'a') as wer_file:\n                wer_file.write(f\"Epoch {epoch}: {epoch_wer}\\n\")\n\n            if (epoch_wer < min_wer):\n                min_wer = epoch_wer\n                torch.save(model, '/kaggle/working/checkpoint_BaseMHA_Linguistic50.pth')\n            print(\"wer checkpoint \" + str(epoch) + \": \" + str(epoch_wer))\n            print(\"min_wer: \" + str(min_wer))\n\n        print(f\"Eval loss: {sum(running_loss) / len(running_loss)}\")\n\n    print(f\"FINISH EPOCH {epoch} IN: {time.time() - t1}\")\n\nloss_file.close()\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2023-06-12T06:46:27.056529Z","iopub.execute_input":"2023-06-12T06:46:27.056895Z"},"trusted":true},"execution_count":null,"outputs":[]}]}